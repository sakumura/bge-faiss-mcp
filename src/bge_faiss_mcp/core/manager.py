"""
HybridSearchManager - ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã¨è»½é‡æ¤œç´¢ã®çµ±åˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼

2ã¤ã®æ¤œç´¢ã‚·ã‚¹ãƒ†ãƒ ã‚’çµ±åˆã—ã€ã‚¯ã‚¨ãƒªã«å¿œã˜ã¦æœ€é©ãªã‚·ã‚¹ãƒ†ãƒ ã‚’é¸æŠã€‚
ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ï¼ˆBGE-M3ï¼‰ã¨è»½é‡æ¤œç´¢ï¼ˆMiniLMï¼‰ã‚’åŠ¹ç‡çš„ã«ä½¿ã„åˆ†ã‘ã‚‹ã€‚
"""

import logging
import os
import json
from typing import List, Dict, Optional, Union, Any
from dataclasses import dataclass
from enum import Enum
from pathlib import Path

from bge_faiss_mcp.core.retriever import SemanticRetriever
from bge_faiss_mcp.core.rag import RAGChain
from bge_faiss_mcp.utils.analyzer import QueryAnalyzer

logger = logging.getLogger(__name__)


class SearchMode(Enum):
    """æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ã®å®šç¾©"""

    SEMANTIC = "semantic"  # BGE-M3ã«ã‚ˆã‚‹é«˜ç²¾åº¦ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢
    PATTERN = "pattern"  # æ­£è¦è¡¨ç¾ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œç´¢
    AUTO = "auto"  # è‡ªå‹•é¸æŠ


@dataclass
class SearchResult:
    """çµ±ä¸€æ¤œç´¢çµæœãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""

    content: str
    score: float
    metadata: Dict[str, Any]
    source: str  # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    mode: SearchMode  # ä½¿ç”¨ã—ãŸæ¤œç´¢ãƒ¢ãƒ¼ãƒ‰


class HybridSearchManager:
    """
    ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã¨ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œç´¢ã‚’çµ±åˆç®¡ç†ã™ã‚‹ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼

    ã‚¯ã‚¨ãƒªã®è¤‡é›‘åº¦ã«å¿œã˜ã¦æœ€é©ãªæ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã‚’è‡ªå‹•é¸æŠã—ã€
    çµ±ä¸€ã•ã‚ŒãŸã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§æ¤œç´¢çµæœã‚’æä¾›ã™ã‚‹ã€‚
    """

    def __init__(
        self,
        semantic_index_path: Optional[str] = None,
        default_mode: SearchMode = SearchMode.AUTO,
        enable_cache: bool = True,
    ):
        """
        Args:
            semantic_index_path: BGE-M3ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒ‘ã‚¹
            default_mode: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰
            enable_cache: æ¤œç´¢çµæœã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã‹
        """
        # Convert to Path objects for consistent handling
        self.semantic_index_path = (
            Path(semantic_index_path)
            if semantic_index_path
            else Path(".search/vectors")
        )
        self.default_mode = default_mode
        self.enable_cache = enable_cache

        # å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®åˆæœŸåŒ–
        self.query_analyzer = QueryAnalyzer()
        self.semantic_retriever = None
        self.rag_chain = None

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self._cache = {} if enable_cache else None

        # æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã®åˆæœŸåŒ–
        self._initialize_engines()

        logger.info(f"HybridSearchManager initialized with mode: {default_mode}")

    def _initialize_engines(self):
        """æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã®é…å»¶åˆæœŸåŒ–"""
        logger.info("ğŸ”§ Starting engine initialization...")

        # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã®åˆæœŸåŒ–ï¼ˆå¸¸ã«å®Ÿè¡Œï¼‰
        try:
            logger.info("ğŸ“¦ Importing semantic search components...")
            from bge_faiss_mcp.core.embedder import BGE_M3Embedder
            from bge_faiss_mcp.core.vector_store import FAISSVectorStore

            logger.info("âœ… Semantic imports successful")

            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            logger.info(
                f"ğŸ“ Creating semantic index directory: {self.semantic_index_path}"
            )
            self.semantic_index_path.mkdir(parents=True, exist_ok=True)
            logger.info("âœ… Directory created successfully")

            # GPU/CPUè‡ªå‹•é¸æŠ
            logger.info("ğŸš€ Initializing BGE-M3 embedder...")
            embedder = BGE_M3Embedder(
                model_name="BAAI/bge-m3", use_fp16=True, batch_size=32
            )
            logger.info("âœ… BGE-M3 embedder initialized")

            # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢åˆæœŸåŒ–
            logger.info("ğŸ—„ï¸ Initializing FAISS vector store...")
            dimension = embedder.get_embedding_dimension()
            logger.info(f"ğŸ“ Embedding dimension: {dimension}")
            vector_store = FAISSVectorStore(
                dimension=dimension,
                store_path=self.semantic_index_path,
                index_type="IVF",  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯IVFï¼ˆå¾Œã§å‹•çš„ã«èª¿æ•´ã•ã‚Œã‚‹ï¼‰
                nlist=100,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼ˆå¾Œã§å‹•çš„ã«èª¿æ•´ã•ã‚Œã‚‹ï¼‰
            )
            logger.info("âœ… FAISS vector store initialized")

            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒ­ãƒ¼ãƒ‰ï¼ˆæ—¢å­˜ã®å ´åˆï¼‰
            if (self.semantic_index_path / "index.faiss").exists():
                vector_store.load(self.semantic_index_path)
                logger.info("Semantic index loaded successfully")
            else:
                logger.info("No existing index found, will create on first search")

            # RetrieveråˆæœŸåŒ–
            self.semantic_retriever = SemanticRetriever(
                embedder=embedder, vector_store=vector_store
            )

            # RAGãƒã‚§ãƒ¼ãƒ³åˆæœŸåŒ–
            self.rag_chain = RAGChain(
                retriever=self.semantic_retriever, max_context_length=2000
            )

            logger.info("Semantic search engine initialized")

        except Exception as e:
            logger.error(f"âŒ Failed to initialize semantic search: {e}")
            import traceback

            logger.error(f"ğŸ“‹ Full traceback: {traceback.format_exc()}")
            self.semantic_retriever = None

        logger.info("âœ… Hybrid search manager initialization complete")

    def search(
        self,
        query: str,
        k: int = 5,
        mode: Optional[SearchMode] = None,
        rerank: bool = True,
    ) -> List[SearchResult]:
        """
        çµ±åˆæ¤œç´¢ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹

        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: å–å¾—ã™ã‚‹çµæœæ•°
            mode: æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ï¼ˆNone ã®å ´åˆã¯è‡ªå‹•é¸æŠï¼‰
            rerank: ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ã‚’å®Ÿè¡Œã™ã‚‹ã‹

        Returns:
            çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®æ¤œç´¢çµæœãƒªã‚¹ãƒˆ
        """
        # åˆå›æ¤œç´¢æ™‚ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒç©ºãªã‚‰æ§‹ç¯‰
        if self.semantic_retriever and not self._has_index():
            logger.info("No index found, building initial index...")
            self.build_initial_index()
        # æ—¢å­˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãŒã‚ã‚‹å ´åˆã€ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´ã‚’ãƒã‚§ãƒƒã‚¯
        elif self.semantic_retriever and self._check_files_changed():
            logger.info("File changes detected, updating index...")
            self.build_initial_index()

        # æ¤œç´¢ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã‚’å¢—åŠ ï¼ˆãƒˆãƒªã‚¬ãƒ¼åˆ¤å®šç”¨ï¼‰
        if self.semantic_retriever and self.semantic_retriever.vector_store:
            self.semantic_retriever.vector_store.increment_search_count()

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        cache_key = f"{query}_{k}_{mode}_{rerank}"
        if self._cache is not None and cache_key in self._cache:
            logger.debug(f"Cache hit for query: {query}")
            return self._cache[cache_key]

        # æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ã®æ±ºå®š
        if mode is None:
            mode = self._determine_search_mode(query)

        logger.info(f"Searching with mode: {mode.value} for query: {query}")

        # æ¤œç´¢å®Ÿè¡Œ
        results = []

        if mode == SearchMode.SEMANTIC and self.semantic_retriever:
            results = self._semantic_search(query, k, rerank)
        elif mode == SearchMode.PATTERN:
            results = self._pattern_search(query, k)
        elif mode == SearchMode.AUTO:
            # è‡ªå‹•ãƒ¢ãƒ¼ãƒ‰ã§ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆ¦ç•¥
            results = self._auto_search(query, k, rerank)
        else:
            logger.warning(f"Search mode {mode} not available")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            results = self._fallback_search(query, k)

        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
        if self._cache is not None:
            self._cache[cache_key] = results

        return results

    def _determine_search_mode(self, query: str) -> SearchMode:
        """ã‚¯ã‚¨ãƒªã‚’åˆ†æã—ã¦æœ€é©ãªæ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ã‚’æ±ºå®š"""
        if self.default_mode != SearchMode.AUTO:
            return self.default_mode

        # ã‚¯ã‚¨ãƒªåˆ†æ
        analysis = self.query_analyzer.analyze(query)

        # åˆ†æçµæœã«åŸºã¥ã„ã¦ãƒ¢ãƒ¼ãƒ‰é¸æŠ
        if analysis.needs_semantic_understanding:
            if self.semantic_retriever:
                return SearchMode.SEMANTIC
        elif analysis.is_simple_keyword:
            if self.semantic_retriever:
                return SearchMode.SEMANTIC
        elif analysis.has_regex_pattern:
            return SearchMode.PATTERN

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢
        return SearchMode.SEMANTIC if self.semantic_retriever else SearchMode.PATTERN

    def _semantic_search(self, query: str, k: int, rerank: bool) -> List[SearchResult]:
        """BGE-M3ã«ã‚ˆã‚‹ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢"""
        try:
            raw_results = self.semantic_retriever.search(
                query, k=k * 2 if rerank else k
            )

            # ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°
            if rerank and len(raw_results) > 0:
                # rerankç”¨ã«document textã‚’æŠ½å‡º
                doc_texts = [item.get("content", "") for item in raw_results]
                rerank_results = self.semantic_retriever.rerank(
                    query, doc_texts, top_k=k
                )

                # rerankã®çµæœã‚’å…ƒã®dictå½¢å¼ã«æˆ»ã™
                reranked_raw_results = []
                for idx, score in rerank_results:
                    if idx < len(raw_results):
                        item = raw_results[idx].copy()
                        item["score"] = score  # rerank scoreã§æ›´æ–°
                        reranked_raw_results.append(item)
                raw_results = reranked_raw_results

            # çµæœã‚’çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›
            results = []
            for item in raw_results[:k]:
                # itemã¯dictå½¢å¼ã®ã¯ãš
                if isinstance(item, dict):
                    result = SearchResult(
                        content=item.get("content", ""),
                        score=item.get("score", 0.0),
                        metadata=item.get("metadata", {}),
                        source=item.get("metadata", {}).get("source", ""),
                        mode=SearchMode.SEMANTIC,
                    )
                    results.append(result)
                else:
                    logger.error(f"Unexpected result format: {type(item)} - {item}")

            return results

        except Exception as e:
            logger.error(f"Semantic search failed: {e}")
            import traceback

            logger.error(f"Full traceback: {traceback.format_exc()}")
            return []

    def _pattern_search(self, query: str, k: int) -> List[SearchResult]:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œç´¢ï¼ˆå°†æ¥çš„ã«Serenaã®search_for_patternã¨é€£æºï¼‰"""
        logger.info("Pattern search not yet implemented, falling back")
        return []

    def _auto_search(self, query: str, k: int, rerank: bool) -> List[SearchResult]:
        """
        è‡ªå‹•ãƒ¢ãƒ¼ãƒ‰ã§ã®æ¤œç´¢ï¼ˆå„ªå…ˆé †ä½ã«å¾“ã£ã¦å®Ÿè¡Œï¼‰
        1. ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢
        2. è»½é‡ç‰ˆæ¤œç´¢
        3. ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œç´¢
        """
        # ã¾ãšã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã‚’è©¦ã™
        if self.semantic_retriever:
            results = self._semantic_search(query, k, rerank)
            if results:
                return results

        # æœ€å¾Œã«ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œç´¢
        results = self._pattern_search(query, k)
        return results

    def _fallback_search(self, query: str, k: int) -> List[SearchResult]:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ¤œç´¢ï¼ˆä½•ã‚‚åˆ©ç”¨ã§ããªã„å ´åˆï¼‰"""
        logger.warning("No search engines available, returning empty results")
        return []

    def query_with_context(
        self, query: str, k: int = 5, mode: Optional[SearchMode] = None
    ) -> Dict[str, Any]:
        """
        RAGãƒã‚§ãƒ¼ãƒ³ã‚’ä½¿ç”¨ã—ã¦ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä»˜ãå›ç­”ã‚’ç”Ÿæˆ

        Args:
            query: è³ªå•
            k: æ¤œç´¢ã™ã‚‹æ–‡æ›¸æ•°
            mode: æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰

        Returns:
            å›ç­”ã¨ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’å«ã‚€è¾æ›¸
        """
        if not self.rag_chain:
            # RAGãƒã‚§ãƒ¼ãƒ³ãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯é€šå¸¸ã®æ¤œç´¢
            results = self.search(query, k, mode)
            return {
                "answer": "RAG chain not available",
                "sources": [r.source for r in results],
                "documents": [r.content for r in results],
            }

        # æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ã®æ±ºå®š
        if mode is None:
            mode = self._determine_search_mode(query)

        # ãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ã¦RAGãƒã‚§ãƒ¼ãƒ³ã‚’ä½¿ç”¨
        if mode == SearchMode.SEMANTIC:
            return self.rag_chain.answer_with_sources(query, k=k)
        else:
            # ä»–ã®ãƒ¢ãƒ¼ãƒ‰ã§ã¯é€šå¸¸ã®æ¤œç´¢çµæœã‚’è¿”ã™
            results = self.search(query, k, mode)
            return {
                "answer": None,
                "sources": [r.source for r in results],
                "documents": [r.content for r in results],
            }

    def _scan_project_files(self, max_files: int = 1000) -> List[Path]:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³"""
        import os
        from pathlib import Path

        cwd = Path.cwd()
        files = []

        # é™¤å¤–ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        exclude_dirs = {
            ".git",
            ".search",
            "__pycache__",
            "node_modules",
            ".venv",
            "venv",
            "env",
            ".env",
            "dist",
            "build",
            ".pytest_cache",
            ".mypy_cache",
        }

        # å¯¾è±¡ã¨ã™ã‚‹æ‹¡å¼µå­
        include_extensions = {
            ".py",
            ".js",
            ".ts",
            ".jsx",
            ".tsx",
            ".md",
            ".txt",
            ".json",
            ".yaml",
            ".yml",
            ".html",
            ".css",
            ".scss",
            ".java",
            ".c",
            ".cpp",
            ".h",
            ".hpp",
            ".go",
            ".rs",
            ".rb",
            ".php",
        }

        for root, dirs, filenames in os.walk(cwd):
            # é™¤å¤–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ã‚¹ã‚­ãƒƒãƒ—
            dirs[:] = [d for d in dirs if d not in exclude_dirs]

            for filename in filenames:
                if len(files) >= max_files:
                    break

                file_path = Path(root) / filename

                # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯ï¼ˆ10MBä»¥ä¸‹ï¼‰
                if file_path.stat().st_size > 10 * 1024 * 1024:
                    continue

                # æ‹¡å¼µå­ãƒã‚§ãƒƒã‚¯
                if file_path.suffix.lower() in include_extensions:
                    files.append(file_path)

        logger.info(f"Found {len(files)} files to index")
        return files

    def _read_file(self, file_path: Path) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        try:
            with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
                content = f.read()
                # æœ€å¤§æ–‡å­—æ•°åˆ¶é™ï¼ˆ100KBï¼‰
                if len(content) > 100000:
                    content = content[:100000]
                return content
        except Exception as e:
            logger.warning(f"Failed to read {file_path}: {e}")
            return ""

    def build_initial_index(self) -> bool:
        """Build initial index"""
        if not self.semantic_retriever:
            logger.warning("Semantic retriever not initialized")
            return False

        try:
            logger.info("Building initial index...")

            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³
            files = self._scan_project_files()

            if not files:
                logger.warning("No files found to index")
                return False

            logger.info(f"Found {len(files)} files to index")

            # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã«åŸºã¥ã„ã¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚¿ã‚¤ãƒ—ã‚’å‹•çš„ã«èª¿æ•´
            if len(files) < 100:
                logger.info(
                    f"Small dataset detected ({len(files)} files), reinitializing with optimized settings"
                )
                # å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ã«å†åˆæœŸåŒ–
                self._reinitialize_for_small_dataset(len(files))

            # ãƒãƒƒãƒå‡¦ç†ç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
            texts = []
            ids = []
            metadata = []

            for file_path in files:
                content = self._read_file(file_path)
                if content:
                    # ãƒ•ã‚¡ã‚¤ãƒ«åã¨å†…å®¹ã‚’çµåˆ
                    relative_path = file_path.relative_to(Path.cwd())
                    text = f"File: {relative_path}\n\n{content}"

                    texts.append(text)
                    ids.append(str(relative_path))
                    metadata.append(
                        {
                            "source": str(relative_path),
                            "file_type": file_path.suffix,
                            "size": len(content),
                        }
                    )

            if texts:
                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«è¿½åŠ 
                logger.info(f"Indexing {len(texts)} documents...")
                self.semantic_retriever.add_documents(
                    texts=texts,
                    ids=ids,
                    metadata=metadata,
                    batch_size=16,  # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å°ã•ã‚ã«
                )

                # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿å­˜
                self.semantic_retriever.vector_store.save()
                logger.info(f"Initial index built with {len(texts)} documents")
                return True
            else:
                logger.warning("No valid content found to index")
                return False

        except Exception as e:
            logger.error(f"Failed to build initial index: {e}")
            import traceback

            logger.error(f"Traceback: {traceback.format_exc()}")
            return False

    def _reinitialize_for_small_dataset(self, file_count: int):
        """Reinitialize vector store for small datasets"""
        try:
            from bge_faiss_mcp.core.embedder import BGE_M3Embedder
            from bge_faiss_mcp.core.vector_store import FAISSVectorStore

            # æ—¢å­˜ã®embedderã‚’ä¿æŒ
            embedder = self.semantic_retriever.embedder

            # å°è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ã«æœ€é©åŒ–ã•ã‚ŒãŸãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆ
            dimension = embedder.get_embedding_dimension()

            # ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã«åŸºã¥ã„ã¦é©åˆ‡ãªè¨­å®šã‚’é¸æŠ
            if file_count < 10:
                # éå¸¸ã«å°ã•ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: Flatã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
                index_type = "Flat"
                nlist = 1
            elif file_count < 50:
                # å°ã•ã„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: å°ã•ã„nlistã§IVF
                index_type = "IVF"
                nlist = 4
            else:
                # ä¸­è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: èª¿æ•´ã•ã‚ŒãŸnlist
                index_type = "IVF"
                nlist = min(10, file_count // 10)

            logger.info(
                f"Reinitializing vector store for {file_count} files with {index_type} index (nlist={nlist})"
            )

            # æ–°ã—ã„ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆ
            vector_store = FAISSVectorStore(
                dimension=dimension,
                store_path=self.semantic_index_path,
                index_type=index_type,
                nlist=nlist,
            )

            # expected_vectors ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§æœ€é©åŒ–ã‚’å‘¼ã³å‡ºã™
            vector_store._initialize_index(expected_vectors=file_count)

            # Retrieverã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’æ›´æ–°
            self.semantic_retriever.vector_store = vector_store
            logger.info(f"Vector store reinitialized successfully for small dataset")

        except Exception as e:
            logger.error(f"Failed to reinitialize for small dataset: {e}")
            # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ãã®ã¾ã¾ç¶šè¡Œï¼ˆæ—¢å­˜ã®è¨­å®šã‚’ä½¿ç”¨ï¼‰

    def _check_files_changed(self) -> bool:
        """Check if project files have changed since last scan."""
        if not self.semantic_retriever or not self.semantic_retriever.vector_store:
            return False

        try:
            import time
            import os
            from pathlib import Path

            metadata = self.semantic_retriever.vector_store.index_metadata
            last_scan_time = metadata.get("last_scan_time", 0)
            search_count = metadata.get("search_count", 0)
            update_strategy = metadata.get("update_strategy", "auto")

            # Manual mode: never auto-update
            if update_strategy == "manual":
                return False

            # Time-based check (default: 1 hour)
            if update_strategy == "time":
                time_threshold = metadata.get("time_threshold", 3600)  # 1 hour
                if time.time() - last_scan_time > time_threshold:
                    logger.info(
                        f"Time threshold exceeded ({time_threshold}s), checking for changes"
                    )
                    return self._scan_for_file_changes(last_scan_time)
                return False

            # Count-based check (default: every 10 searches)
            if update_strategy == "count":
                count_threshold = metadata.get("count_threshold", 10)
                if search_count >= count_threshold:
                    logger.info(
                        f"Search count threshold reached ({search_count}/{count_threshold})"
                    )
                    # Reset counter
                    metadata["search_count"] = 0
                    return self._scan_for_file_changes(last_scan_time)
                return False

            # Auto mode: intelligent checks based on project size
            file_count = metadata.get("file_count", 0)
            if file_count < 100:
                # Small projects: check every 5 searches
                if search_count >= 5:
                    metadata["search_count"] = 0
                    return self._scan_for_file_changes(last_scan_time)
            elif file_count < 1000:
                # Medium projects: check every 15 searches
                if search_count >= 15:
                    metadata["search_count"] = 0
                    return self._scan_for_file_changes(last_scan_time)
            else:
                # Large projects: time-based (30 minutes)
                if time.time() - last_scan_time > 1800:
                    return self._scan_for_file_changes(last_scan_time)

            return False

        except Exception as e:
            logger.warning(f"Failed to check file changes: {e}")
            return False

    def _scan_for_file_changes(self, last_scan_time: float) -> bool:
        """Scan project files for changes since last_scan_time."""
        try:
            import os
            from pathlib import Path

            # Get current project files
            current_files = self._scan_project_files()

            # Check for new or modified files
            changes_detected = False

            for file_path in current_files:
                try:
                    file_mtime = file_path.stat().st_mtime
                    if file_mtime > last_scan_time:
                        logger.debug(f"File changed: {file_path}")
                        changes_detected = True
                        break  # Early exit for performance
                except (OSError, FileNotFoundError):
                    # File might have been deleted or inaccessible
                    continue

            if changes_detected:
                logger.info("File changes detected")
                return True

            # Check if files were deleted (current count vs indexed count)
            metadata = self.semantic_retriever.vector_store.index_metadata
            indexed_count = metadata.get("file_count", 0)
            current_count = len(current_files)

            if current_count != indexed_count:
                logger.info(f"File count changed: {indexed_count} -> {current_count}")
                return True

            logger.debug("No file changes detected")
            return False

        except Exception as e:
            logger.warning(f"Failed to scan for file changes: {e}")
            return False

    def _has_index(self) -> bool:
        """Check if index exists"""
        if not self.semantic_retriever:
            return False

        stats = self.semantic_retriever.vector_store.get_stats()
        return stats.get("num_documents", 0) > 0

    def clear_cache(self):
        """æ¤œç´¢ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢"""
        if self._cache is not None:
            self._cache.clear()
            logger.info("Search cache cleared")

    def get_available_modes(self) -> List[SearchMode]:
        """åˆ©ç”¨å¯èƒ½ãªæ¤œç´¢ãƒ¢ãƒ¼ãƒ‰ã‚’å–å¾—"""
        modes = [SearchMode.AUTO]

        if self.semantic_retriever:
            modes.append(SearchMode.SEMANTIC)

        # ãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œç´¢ã¯å¸¸ã«åˆ©ç”¨å¯èƒ½ï¼ˆå°†æ¥å®Ÿè£…ï¼‰
        modes.append(SearchMode.PATTERN)

        return modes

    def get_stats(self) -> Dict[str, Any]:
        """æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        stats = {
            "available_modes": [m.value for m in self.get_available_modes()],
            "default_mode": self.default_mode.value,
            "cache_enabled": self.enable_cache,
            "cache_size": len(self._cache) if self._cache else 0,
        }

        # ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯æ¤œç´¢ã®çµ±è¨ˆ
        if self.semantic_retriever:
            stats["semantic"] = self.semantic_retriever.get_stats()

        return stats


# ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
__all__ = ["HybridSearchManager", "SearchMode", "SearchResult"]
